#!/bin/bash
#SBATCH --account=PAS2136
#SBATCH --job-name=segment_test
#SBATCH --time=00:10:00
#
# Runs Snakemake pipeline that downloads, crops, and segments images.
# This script has three positional arguments. The first two positional
# arguments are required.
#
# Usage:
# sbatch SLURM_Snake <WORKDIR> <INPUTCSV> [NUMJOBS]
# - WORKDIR - Snakemake working directory - contains output files
# - INPUTCSV - Path to input CSV file specifying images to process
# - NUMJOBS - Optional number of jobs Snakemake will run at once, defaults to 4

# Stop if a command fails (non-zero exit status)
set -e

# Verify command line arguments
WORKDIR=$1
INPUTCSV=$2
NUMJOBS=$3

# Default to running 4 jobs if not set
if [ -z "$NUMJOBS" ]
then
      NUMJOBS=4
fi

# Ensures CSV path is an absolute path.
# Otherwise snakemake will look for this file within $WORKDIR.
REAL_INPUTCSV=$(realpath $INPUTCSV)

# Make sure the input CSV file exists
if [ ! -f "$REAL_INPUTCSV" ]
then
   echo "ERROR: Required INPUTCSV file $INPUTCSV does not exist."
   exit 1
fi

# Specify cache directories
CACHEDIR=$(realpath .cache)
# cache singularity images
SINGULARITY_IMAGE_DIR=$CACHEDIR/singularity
# cache pytorch files
export TORCH_HOME=$CACHEDIR/torch
# ensure pytorch cache directory exists
mkdir -p $TORCH_HOME

# Setup Snakemake/sbatch to use same account as this job
export SBATCH_ACCOUNT=$SLURM_JOB_ACCOUNT

# Activate Snakemake environment
module load miniconda3/4.10.3-py37
# Activate using source per OSC instructions
source activate snakemake

# Run pipeline using Snakemake
snakemake \
    --jobs $NUMJOBS \
    --profile slurm/ \
    --use-singularity \
    --singularity-prefix $SINGULARITY_IMAGE_DIR \
    --singularity-args "--bind $TORCH_HOME:$TORCH_HOME" \
    --directory $WORKDIR \
    --config list=$REAL_INPUTCSV

